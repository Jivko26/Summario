# -*- coding: utf-8 -*-
"""translation_fromtxt

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16kMo6xCv5Uofd624mEn8jqFJ8LdY1XGt
"""

!pip install ctranslate2
!pip install sentencepiece

# Define the content for the text file
content = """Abstract
The European Union is a great source of high quality documents with translations into several languages. Parallel corpora from its
publications are frequently used in various tasks, machine translation in particular. A source that has not systematically been explored
yet is the EU Bookshop â€“ an online service and archive of publications from various European institutions. The service contains a large
body of publications in the 24 official of the EU. This paper describes our efforts in collecting those publications and converting them
to a format that is useful for natural language processing in particular statistical machine translation. We report our procedure of
crawling the website and various pre-processing steps that were necessary to clean up the data after the conversion from the original
PDF files. Furthermore, we demonstrate the use of this dataset in training SMT models for English, French, German, Spanish, and
Latvian.
"""


# Define the file path where the text will be saved
file_path = '/content/drive/MyDrive/nmt/input_text.txt'

# Write the larger text to a file
with open(file_path, 'w') as file:
    file.write(content)

with open(file_path, 'r') as file:
    content = file.read()
    print(content)

print(f"File '{file_path}' with input text has been created.")

import ctranslate2
import sentencepiece as spm

# Set file paths
source_file_path = "/content/drive/MyDrive/nmt/input_text.txt"
target_file_path = "/content/drive/MyDrive/nmt/output.txt"

sp_source_model_path = "/content/drive/MyDrive/nmt/source.model"
sp_target_model_path = "/content/drive/MyDrive/nmt/target.model"

ct_model_path = "/content/drive/MyDrive/nmt/nlen_ctranslate_new"

# Load the source SentecePiece model
sp = spm.SentencePieceProcessor()
sp.load(sp_source_model_path)

# Open the source file
with open(source_file_path, "r") as source:
  lines = source.readlines()

source_sents = [line.strip() for line in lines]

# Subword the source sentences
source_sents_subworded = sp.encode_as_pieces(source_sents)

# Translate the source sentences
translator = ctranslate2.Translator(ct_model_path, device="cuda")  # or "cuda" for GPU
translations = translator.translate_batch(source_sents_subworded, batch_type="tokens", max_batch_size=4096)
translations = [translation.hypotheses[0] for translation in translations]

# Load the target SentecePiece model
sp.load(sp_target_model_path)

# Desubword the target sentences
translations_desubword = sp.decode(translations)

# Save the translations to the a file
with open(target_file_path, "w+", encoding="utf-8") as target:
  for line in translations_desubword:
    target.write(line.strip() + "\n")

print("Done")

print ("Source EN: ")
with open(source_file_path, 'r', encoding='utf-8') as file:
    content = file.read()
    print(content)


print ("Translation NL: ")
with open(target_file_path, 'r', encoding='utf-8') as file:
    content = file.read()
    print(content)